<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>自定义唤醒词检测</title>
    <link rel="stylesheet" href="kws-demo.css">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/plotly.js-dist@2.12.1/plotly.min.js"></script>
    <script src="kws.js"></script>
</head>

<body>
    <div class="container">
        <h1>自定义唤醒词检测器</h1>
        <div class="status-container">
            <div class="status">
                <span class="status-indicator" id="statusIndicator"></span>
                <span id="statusText">正在加载模型...</span>
            </div>
        </div>
        <div class="controls">
            <button id="startBtn" class="btn" disabled>开始监听</button>
            <button id="stopBtn" class="btn" disabled>停止监听</button>
        </div>
        <div class="visualization">
            <canvas id="audioVisualizer" width="600" height="350"></canvas>
            <div id="mel-spectrogram"></div>
            <div id="result-chart"></div>
        </div>
        <div class="detection-log">
            <h3>检测日志</h3>
            <div id="detectionLog"></div>
        </div>
        <div class="settings">
            <h3>设置</h3>
            <div class="setting-item">
                <label for="threshold">检测阈值:</label>
                <input type="range" id="threshold" min="0" max="1" step="0.01" value="0.75">
                <span id="thresholdValue">0.75</span>
            </div>
        </div>
    </div>
    <script>
    // 全局变量
    let model;
    let isListening = false;
    let audioContext;
    let analyser;
    let microphoneStream;
    let detectionThreshold = 0.75;
    let mfccProcessor = null;
    let canvasContext;
    let animationFrameId;
    let bufferSize = 1024;
    let mfccCoefficients = 26;

    // DOM元素
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const statusIndicator = document.getElementById('statusIndicator');
    const statusText = document.getElementById('statusText');
    const detectionLog = document.getElementById('detectionLog');
    const thresholdSlider = document.getElementById('threshold');
    const thresholdValue = document.getElementById('thresholdValue');
    const canvas = document.getElementById('audioVisualizer');
    canvasContext = canvas.getContext('2d');

    // 初始化应用
    async function init() {
        updateStatus('loading', '正在加载唤醒词模型...');

        try {
            // 获取模型JSON并修复输入形状
            // const response = await fetch('kws_js/model.json');
            // const modelJSON = await response.json();

            // // 检查并修复输入层
            // if (modelJSON.modelTopology && 
            //     modelJSON.modelTopology.model_config && 
            //     modelJSON.modelTopology.model_config.config &&
            //     modelJSON.modelTopology.model_config.config.layers &&
            //     modelJSON.modelTopology.model_config.config.layers.length > 0) {

            //     const firstLayer = modelJSON.modelTopology.model_config.config.layers[0];

            //     // 确保第一层有输入形状
            //     if (firstLayer.config && !firstLayer.config.batch_input_shape) {
            //         console.log('修复输入形状...', firstLayer.config.batch_shape);
            //         // 使用适合您模型的形状 - 根据MFCC特征调整
            //         firstLayer.config.batch_input_shape = firstLayer.config.batch_shape;
            //         console.log('已添加输入形状');
            //     }
            // }

            // 使用修复后的JSON加载模型
            model = await tf.loadLayersModel('kws_js/model.json'); //tf.io.fromMemory(modelJSON));
            // model = await tf.loadGraphModel(tf.io.fromMemory(modelJSON));
            console.log('模型加载成功');
            updateStatus('ready', '模型已加载。准备监听。');

            startBtn.disabled = false;
        } catch (error) {
            console.error('加载模型失败:', error);
        }

        // 设置阈值控制
        thresholdSlider.addEventListener('input', function() {
            detectionThreshold = parseFloat(this.value);
            thresholdValue.textContent = detectionThreshold.toFixed(2);
        });

        // 按钮事件监听器
        startBtn.addEventListener('click', startListening);
        stopBtn.addEventListener('click', stopListening);
    }

    // 更新状态UI
    function updateStatus(state, message) {
        statusIndicator.className = 'status-indicator ' + state;
        statusText.textContent = message;
    }

    // 添加日志条目
    function addLogEntry(message, isWakeWord = false) {
        const entry = document.createElement('div');
        entry.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;

        if (isWakeWord) {
            entry.className = 'wake-word-detected';
        }

        detectionLog.appendChild(entry);
        detectionLog.scrollTop = detectionLog.scrollHeight;

        // 只保留最后50条记录
        while (detectionLog.childElementCount > 50) {
            detectionLog.removeChild(detectionLog.firstChild);
        }
    }

    let arrayBuffer = []
    let identifyResult = []
    let melSpec = [[]]

    const WINDOW_SIZE = 200
    const SLIDE_WINDOW_SIZE = 50
    const numOfBatches = 1
    const SPEC_HOP_LENGTH = 256
    const MEL_SPEC_BINS = 40
    const NUM_FFTS = 512
    const LOG_OFFSET = 1e-4

    const wakeWord = "小丁";
    let wakeWordStage = -1;
    let wakeWordStageCounter = 0;

    const windowBufferSize = WINDOW_SIZE / 1000 * SAMPLE_RATE
    const slideWindowBufferSize = SLIDE_WINDOW_SIZE / 1000 * SAMPLE_RATE
    const wakeWordMaxGap = 20;   // Gap = n * slideWindow

    const cpuMel = new MelSpectrogram({
        sampleRate: SAMPLE_RATE,
        hopLength: SPEC_HOP_LENGTH,
        n_mels: MEL_SPEC_BINS,
        nFft: NUM_FFTS,
        winLength: NUM_FFTS,
        f_min: 0,
        f_max: 8000,
        n_stft: Math.floor(NUM_FFTS / 2) + 1,
        norm: 'slaney',
        mel_scale: 'htk'
    });

    function plotMelSpectrogram(melSpec, title, xmax) {
        const layout = {
            title: title,
            xaxis: { title: 'Frame' },
            yaxis: { title: 'Mel Frequency', range: [0, MEL_SPEC_BINS] },
            coloraxis: { colorbar: { title: 'Magnitude (dB)' } },
            margin: {
                t: 30,
                b: 35,
                r: 10
            },
        };

        if (xmax) {
            layout.xaxis.range = [0, xmax];
        }

        Plotly.newPlot('mel-spectrogram', [{ z: melSpec, type: 'heatmap' }], layout);
    }

    function resultChart() {
        const layout = {
            xaxis: { title: 'Time' },
            yaxis: { title: 'Probability', range: [0, 1] },
            margin: {
                t: 10,
                b: 25,
                r: 10
            },
        };

        var data = [];
        for (var i = 0; i < wakeWord.length; i++)
        {
            data.push({
                y: [],
                name: wakeWord[i],
                type: 'scatter'
            });
        }

        Plotly.newPlot('result-chart', data, layout);
    }

    // 启动音频处理和唤醒词检测
    async function startListening() {
        if (isListening) return;

        plotMelSpectrogram(await tf.zeros([40,38]));
        resultChart();

        try {
            // 请求麦克风权限
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

            // 设置音频上下文和分析器
            audioContext = new(window.AudioContext || window.webkitAudioContext)({
                sampleRate: SAMPLE_RATE
            });
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 512;

            // 连接麦克风到分析器
            microphoneStream = audioContext.createMediaStreamSource(stream);
            microphoneStream.connect(analyser);

            // bufferSize, in_channels, out_channels
            scriptNode = (audioContext.createScriptProcessor || audioContext.createJavaScriptNode).call(audioContext, bufferSize, 1, 1);
            scriptNode.onaudioprocess = async function(audioEvent) {
                let resampledMonoAudio = await resampleAndMakeMono(audioEvent.inputBuffer);
                arrayBuffer = [...arrayBuffer, ...resampledMonoAudio]
                batchSize = Math.floor(arrayBuffer.length / windowBufferSize)
                // if we got batches * 750 ms seconds of buffer 
                let batchMels = []
                if (arrayBuffer.length >= numOfBatches * windowBufferSize) {
                    let batch = 0
                    for (; arrayBuffer.length > windowBufferSize;) {
                        batchBuffer = arrayBuffer.slice(0, windowBufferSize)
                        arrayBuffer = arrayBuffer.slice(slideWindowBufferSize)

                        // calculate log mels
                        let mel_result = cpuMel.forward(batchBuffer, LOG_OFFSET);
                        melSpec = mel_result;
                        let log_mels = tf.stack(mel_result).expandDims(2)
                        batchMels.push(log_mels)
                        batch = batch + 1
                    }
                    // arrayBuffer = arrayBuffer.slice((batch - 1) * windowBufferSize + slideWindowBufferSize)
                    // clear buffer

                    // console.log(batchMels);
                    // Run model with Tensor inputs and get the result.
                    let outputTensor = tf.tidy(() => {
                        inputTensor = tf.stack(batchMels)
                        // let inputTensor = tf.tensor(dataProcessed, [batch, MEL_SPEC_BINS, dataProcessed.length/(batch * MEL_SPEC_BINS), 1], 'float32');
                        let outputTensor = model.predict(inputTensor);
                        inputTensor.dispose();
                        return outputTensor
                    });
                    let outputData = await outputTensor.data();
                    batchMels.forEach((mels) => mels.dispose());
                    outputTensor.dispose();

                    for (var batch_ = 0; batch_ < batch; batch_++)
                    {
                        const classSize = outputData.length / batch;
                        const batchData = Array.from(outputData).slice(classSize * batch_, classSize * (batch_ + 1));

                        const time = new Date();
                        var update = {
                            y: []
                        }
                        for (var i = 0; i < batchData.length - 1;i++)
                        {
                            update['y'].push([batchData[i]]);
                        }
                        Plotly.extendTraces('result-chart', update, [0, 1], 100)

                        let probs = softmax(batchData);
                        let probs_sum = probs.reduce( (sum, x) => x+sum);
                        let class_idx = argMax(probs);

                        const isAllWordOverThreshold = batchData.slice(0, -1).reduce((a, b) => b > detectionThreshold ? a+1 : 0, 0) == batchData.length - 1;
                        let isWordOrderMatchThreshold = false;
                        // Match by probe
                        if (class_idx == probs.length - 1)
                        {
                            // Not matched
                            if (wakeWordStage != -1)
                            {
                                wakeWordStageCounter++;
                            }
                            if (wakeWordStageCounter > wakeWordMaxGap)
                            {
                                wakeWordStage = -1;
                                wakeWordStageCounter = 0;
                            }
                        } else {
                            let wakeWordProbability = batchData[class_idx];
                            // match words
                            if (class_idx == wakeWordStage || class_idx == wakeWordStage + 1)
                            {
                                if (wakeWordProbability > detectionThreshold)
                                {
                                    wakeWordStage = class_idx;
                                    wakeWordStageCounter = 0;
                                    isWordOrderMatchThreshold = wakeWordStage == classSize - 2;
                                    if (class_idx == wakeWordStage + 1)
                                    {
                                        addLogEntry(`命中关键字顺序 ${wakeWord[class_idx]}: ${wakeWordProbability.toFixed(4)}`);
                                    }
                                }
                                // console.info(`命中关键字顺序 ${wakeWord[class_idx]}: ${wakeWordProbability}`)
                            } else {
                                // console.info(`错误关键字顺序 ${wakeWord[class_idx]}: ${wakeWordProbability}`)
                                if (wakeWordProbability > detectionThreshold)
                                {
                                    addLogEntry(`错误关键字顺序 ${wakeWord[class_idx]}: ${wakeWordProbability.toFixed(4)}`);
                                }
                            }
                        }

                        if (isWordOrderMatchThreshold || isAllWordOverThreshold)
                        {
                            let wakeWordProbability = batchData.slice(0, -1).reduce((a, b) => b > a ? b : a, 0);
                            console.log(`检测到唤醒词，概率: ${wakeWordProbability.toFixed(4)}`);
                            addLogEntry(`检测到唤醒词！(置信度: ${wakeWordProbability.toFixed(4)})`, true);

                            // 在这里触发对唤醒词的响应
                            triggerWakeWordAction();
                        }
                    }
                }
            }
            microphoneStream.connect(scriptNode);
            scriptNode.connect(audioContext.destination);

            // 更新UI
            isListening = true;

            // 开始可视化
            drawAudioVisualizer();

            updateStatus('listening', '正在监听唤醒词...');
            startBtn.disabled = true;
            stopBtn.disabled = false;

            addLogEntry('开始监听唤醒词');
        } catch (error) {
            console.error('访问麦克风时出错:', error);
            updateStatus('error', '访问麦克风出错');
            addLogEntry(`错误: ${error.message}`);
        }
    }

    // 绘制音频可视化
    function drawAudioVisualizer() {
        if (!isListening) return;

        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        analyser.getByteTimeDomainData(dataArray);

        // 清除画布
        canvasContext.clearRect(0, 0, canvas.width, canvas.height);

        // 设置绘图
        canvasContext.lineWidth = 2;
        canvasContext.strokeStyle = 'rgb(0, 125, 255)';
        canvasContext.beginPath();

        const sliceWidth = canvas.width * 1.0 / bufferLength;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
            const v = dataArray[i] / 128.0;
            const y = v * canvas.height / 2;

            if (i === 0) {
                canvasContext.moveTo(x, y);
            } else {
                canvasContext.lineTo(x, y);
            }

            x += sliceWidth;
        }

        canvasContext.lineTo(canvas.width, canvas.height / 2);
        canvasContext.stroke();

        Plotly.restyle('mel-spectrogram', 'z', [melSpec]);
        animationFrameId = requestAnimationFrame(drawAudioVisualizer);
    }

    // 停止监听
    function stopListening() {
        if (!isListening) return;

        // 停止音频处理
        if (mfccProcessor) {
            mfccProcessor.stop();
        }

        // 停止可视化
        if (animationFrameId) {
            cancelAnimationFrame(animationFrameId);
        }

        // 清理音频资源
        if (microphoneStream) {
            microphoneStream.disconnect();
        }

        if (audioContext) {
            audioContext.close().catch(console.error);
        }

        // 更新UI
        isListening = false;
        updateStatus('ready', '准备监听');
        startBtn.disabled = false;
        stopBtn.disabled = true;

        addLogEntry('停止监听');
    }

    // 在检测到唤醒词时执行的操作
    function triggerWakeWordAction() {
        // 这个函数可以包含您应用的特定响应
        console.log('唤醒词触发动作!');

        // 闪烁状态指示器
        statusIndicator.style.backgroundColor = '#27ae60';
        setTimeout(() => {
            if (isListening) {
                statusIndicator.style.backgroundColor = '';
                statusIndicator.className = 'status-indicator listening';
            }
        }, 1000);

        // 您也可以播放声音、显示通知等
        // 例如，根据唤醒词激活语音助手
        // const audio = new Audio('sounds/activation.mp3');
        // audio.play().catch(console.error);

        // 在此处添加您的自定义逻辑
        // 例如: 启动语音识别、执行某些命令等
    }

    // 当页面加载时初始化应用
    window.addEventListener('load', init);

    // 当页面卸载时清理资源
    window.addEventListener('beforeunload', () => {
        if (isListening) {
            stopListening();
        }
    });
    </script>
</body>

</html>